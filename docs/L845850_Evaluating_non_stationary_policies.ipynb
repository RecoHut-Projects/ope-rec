{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L845850 | Evaluating non-stationary policies",
      "provenance": [],
      "authorship_tag": "ABX9TyNVlQAB8QVDZQ13qWfs7sjJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk8tjyirkiCH"
      },
      "source": [
        "# Evaluating non-stationary policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65j_TlXZkork"
      },
      "source": [
        "Many decision-making algorithms perform online learning, where an internal model is continuously updated. Such policies are nonstationary, because they depend on the history of contexts, as well as the current context. These policies are more difficult to evaluate, because of their interactive nature.\n",
        "\n",
        "The current best practice for evaluating non-stationary policies is calledÂ *doubly robust nonstationary*, as it extends the doubly robust technique. The nonstationarity is handled by replaying the target policy iteratively over the historical data with rejection sampling, which is a way to simulate data from an unknown distribution.\n",
        "\n",
        "The technique works by iterating over the historical data and probabilistically accepting/rejecting instances to include in the synthetic dataset. By performing this rejection sampling iteratively over the historical data, the nonstationary policy under evaluation can be updated for each instance, thereby simulating online learning. In contrast, the techniques for stationary policies in the previous sections involved non-iterative, batch calculations on the entire historical dataset.\n",
        "\n",
        "The rejection sampling is done in proportion to exploration and target action probabilities. Specifically, it tends to reject instances with actions that are more likely in exploration than target policies, so that the distribution of accepted instances more closely matches the target policy, rather than the exploration policy.\n",
        "\n",
        "To minimise the variance in this technique, it efficiently uses all historical instances (accepted or rejected) in the doubly robust reward estimate. These reward estimates are averaged over each round, to further lower the variance. To control the bias in this technique, there are tunable parameters to control the level of bias in the estimate (at the cost of more or less efficient use of the historical data). The result is an evaluator that balances bias and variance, enabling nonstationary policies to be evaluated on historical data.\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://ambiata.com/blog/2020-10-27-off-policy-evaluation/](https://ambiata.com/blog/2020-10-27-off-policy-evaluation/)\n",
        "2. [Doubly Robust Policy Evaluation and Optimization](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/paper-14.pdf)\n",
        "3. [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/pdf/1003.0146.pdf)"
      ]
    }
  ]
}