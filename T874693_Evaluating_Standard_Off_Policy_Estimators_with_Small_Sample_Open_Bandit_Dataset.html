
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset &#8212; ope-rec</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/quantecon-book-theme.1ef59f8f4e91ec8319176e8479c6af4e.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html" />
    <link rel="prev" title="Evaluating a New Fraud Policy with DM, IPW, and DR Methods" href="T601959_Evaluating_a_New_Fraud_Policy_with_DM%2C_IPW%2C_and_DR_Methods.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Sparsh A." />
<meta name="keywords" content="" />
<meta name="description" content=Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset  Overview  Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobu />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset"/>
<meta name="twitter:description" content="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset  Overview  Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobu">
<meta name="twitter:creator" content="@">
<meta name="twitter:image" content="">

<!-- Opengraph tags -->
<meta property="og:title" content="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset" />
<meta property="og:type" content="website" />
<meta property="og:url" content="None" />
<meta property="og:image" content="" />
<meta property="og:description" content="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset  Overview  Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobu" />
<meta property="og:site_name" content="ope-rec" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataloader">
   Dataloader
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ope-estimators">
   OPE Estimators
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#utils">
     Utils
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract-class">
     Abstract Class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-method">
     Direct Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#doubly-robust">
     Doubly Robust
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-probability-weighting">
     Inverse Probability Weighting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#off-policy-evaluation-class">
   Off-Policy Evaluation Class
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-models">
   Base Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Utils
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-model">
     Regression Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policies">
   Policies
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#base-context-free-policy">
     Base Context Free Policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epsilon-greedy">
     Epsilon Greedy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random">
     Random
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bernoullits">
     BernoulliTS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run">
   Run
  </a>
 </li>
</ul>

                            <p class="logo">
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="US773842_Off_Policy_Evaluation.html">ope-rec</a></p>

                        <p class="qe-page__header-subheading">Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset</p>

                    </div>

                    <p class="qe-page__header-authors">Sparsh A.</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <div class="tex2jax_ignore mathjax_ignore section" id="evaluating-standard-off-policy-estimators-with-small-sample-open-bandit-dataset">
<h1>Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset<a class="headerlink" href="#evaluating-standard-off-policy-estimators-with-small-sample-open-bandit-dataset" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p><strong>Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobust, and InverseProbabilityWeighting) on small sample open-bandit dataset</strong>.</p>
<p>These OPE estimators will estimate the performance of BernoulliTS policy (counterfactual/evaluation policy) using data generated by Random policy (behavior policy).</p>
<p>Imports</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">yaml</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dataloader">
<h2>Dataloader<a class="headerlink" href="#dataloader" title="Permalink to this headline">¶</a></h2>
<p>Abstract Base Class for Logged Bandit Feedback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseBanditDataset</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Synthetic Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseRealBanditDataset</span><span class="p">(</span><span class="n">BaseBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base Class for Real-World Bandit Dataset.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess raw dataset.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
<p>Dataset Class for Real-World Logged Bandit Feedback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">basicConfig</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">INFO</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset type</span>
<span class="n">BanditFeedback</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OBD_DATA_PATH</span> <span class="o">=</span> <span class="s1">&#39;/content/zr-obp/obd&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">OpenBanditDataset</span><span class="p">(</span><span class="n">BaseRealBanditDataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class for loading and preprocessing Open Bandit Dataset.</span>
<span class="sd">    Note</span>
<span class="sd">    -----</span>
<span class="sd">    Users are free to implement their own feature engineering by overriding the `pre_process` method.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    behavior_policy: str</span>
<span class="sd">        Name of the behavior policy that generated the logged bandit feedback data.</span>
<span class="sd">        Must be either &#39;random&#39; or &#39;bts&#39;.</span>
<span class="sd">    campaign: str</span>
<span class="sd">        One of the three possible campaigns considered in ZOZOTOWN.</span>
<span class="sd">        Must be one of &quot;all&quot;, &quot;men&quot;, or &quot;women&quot;.</span>
<span class="sd">    data_path: str or Path, default=None</span>
<span class="sd">        Path where the Open Bandit Dataset is stored.</span>
<span class="sd">    dataset_name: str, default=&#39;obd&#39;</span>
<span class="sd">        Name of the dataset.</span>
<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Yuta Saito, Shunsuke Aihara, Megumi Matsutani, Yusuke Narita.</span>
<span class="sd">    &quot;Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">behavior_policy</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">campaign</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">data_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;obd&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Open Bandit Dataset Class.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;bts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;random&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;behavior_policy must be either of &#39;bts&#39; or &#39;random&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;all&quot;</span><span class="p">,</span>
            <span class="s2">&quot;men&quot;</span><span class="p">,</span>
            <span class="s2">&quot;women&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;campaign must be one of &#39;all&#39;, &#39;men&#39;, or &#39;women&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">OBD_DATA_PATH</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_path must be a string or Path&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">behavior_policy</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_data_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="si">}</span><span class="s2">.csv&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_raw_data</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_process</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_rounds</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Total number of rounds contained in the logged bandit dataset.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Number of actions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dim_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Dimensions of context vectors.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">len_list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Length of recommendation lists.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">calc_on_policy_policy_value_estimate</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">behavior_policy</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">campaign</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">data_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Calculate on-policy policy value estimate (used as a ground-truth policy value).</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        behavior_policy: str</span>
<span class="sd">            Name of the behavior policy that generated the log data.</span>
<span class="sd">            Must be either &#39;random&#39; or &#39;bts&#39;.</span>
<span class="sd">        campaign: str</span>
<span class="sd">            One of the three possible campaigns considered in ZOZOTOWN (i.e., &quot;all&quot;, &quot;men&quot;, and &quot;women&quot;).</span>
<span class="sd">        data_path: Path, default=None</span>
<span class="sd">            Path where the Open Bandit Dataset exists.</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data by time series.</span>
<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        on_policy_policy_value_estimate: float</span>
<span class="sd">            Policy value of the behavior policy estimated by on-policy estimation, i.e., :math:`\\mathbb{E}_{\\mathcal{D}} [r_t]`.</span>
<span class="sd">            where :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">            This parameter is used as a ground-truth policy value in the evaluation of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">behavior_policy</span><span class="o">=</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">data_path</span>
        <span class="p">)</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
            <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="n">bandit_feedback</span>
        <span class="k">return</span> <span class="n">bandit_feedback_test</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load raw open bandit dataset.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_data_file</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;item_context.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;timestamp&quot;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;item_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position</span> <span class="o">=</span> <span class="p">(</span><span class="n">rankdata</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="nb">int</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;click&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pscore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;propensity_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess raw open bandit dataset.</span>
<span class="sd">        Note</span>
<span class="sd">        -----</span>
<span class="sd">        This is the default feature engineering and please override this method to</span>
<span class="sd">        implement your own preprocessing.</span>
<span class="sd">        see https://github.com/st-tech/zr-obp/blob/master/examples/examples_with_obd/custom_dataset.py for example.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">user_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&quot;user_feature&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">user_cols</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="n">item_feature_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span><span class="p">[</span><span class="s2">&quot;item_feature_0&quot;</span><span class="p">]</span>
        <span class="n">item_feature_cat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_context</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;item_feature_0&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">item_feature_cat</span><span class="p">,</span> <span class="n">item_feature_0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

    <span class="k">def</span> <span class="nf">obtain_batch_bandit_feedback</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">BanditFeedback</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">BanditFeedback</span><span class="p">,</span> <span class="n">BanditFeedback</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Obtain batch logged bandit feedback.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data into train and test sets based on time series.</span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        bandit_feedback: BanditFeedback</span>
<span class="sd">            A dictionary containing batch logged bandit feedback data collected by a behavior policy.</span>
<span class="sd">            The keys of the dictionary are as follows.</span>
<span class="sd">            - n_rounds: number of rounds (size) of the logged bandit data</span>
<span class="sd">            - n_actions: number of actions (:math:`|\mathcal{A}|`)</span>
<span class="sd">            - action: action variables sampled by a behavior policy</span>
<span class="sd">            - position: positions where actions are recommended</span>
<span class="sd">            - reward: reward variables</span>
<span class="sd">            - pscore: action choice probabilities by a behavior policy</span>
<span class="sd">            - context: context vectors such as user-related features and user-item affinity scores</span>
<span class="sd">            - action_context: item-related context vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">is_timeseries_split</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`is_timeseries_split` must be a bool, but </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">is_timeseries_split</span><span class="p">)</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">check_scalar</span><span class="p">(</span>
                <span class="n">test_size</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;target_size&quot;</span><span class="p">,</span>
                <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span>
                <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">n_rounds_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">test_size</span><span class="p">))</span>
            <span class="n">bandit_feedback_train</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds_train</span><span class="p">,</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">[:</span><span class="n">n_rounds_train</span><span class="p">],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">bandit_feedback_test</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span> <span class="o">-</span> <span class="n">n_rounds_train</span><span class="p">),</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="n">n_rounds_train</span><span class="p">:],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">bandit_feedback_train</span><span class="p">,</span> <span class="n">bandit_feedback_test</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">n_rounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_rounds</span><span class="p">,</span>
                <span class="n">n_actions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position</span><span class="p">,</span>
                <span class="n">reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_bootstrap_bandit_feedback</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="n">is_timeseries_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BanditFeedback</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Obtain bootstrap logged bandit feedback.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        sample_size: int, default=None</span>
<span class="sd">            Number of data sampled by bootstrap.</span>
<span class="sd">            When None is given, the original data size (n_rounds) is used as `sample_size`.</span>
<span class="sd">            The value must be smaller than the original data size.</span>
<span class="sd">        test_size: float, default=0.3</span>
<span class="sd">            Proportion of the dataset included in the test split.</span>
<span class="sd">            If float, should be between 0.0 and 1.0.</span>
<span class="sd">            This argument matters only when `is_timeseries_split=True` (the out-sample case).</span>
<span class="sd">        is_timeseries_split: bool, default=False</span>
<span class="sd">            If true, split the original logged bandit feedback data into train and test sets based on time series.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        bandit_feedback: BanditFeedback</span>
<span class="sd">            A dictionary containing logged bandit feedback data sampled independently from the original data with replacement.</span>
<span class="sd">            The keys of the dictionary are as follows.</span>
<span class="sd">            - n_rounds: number of rounds (size) of the logged bandit data</span>
<span class="sd">            - n_actions: number of actions</span>
<span class="sd">            - action: action variables sampled by a behavior policy</span>
<span class="sd">            - position: positions where actions are recommended by a behavior policy</span>
<span class="sd">            - reward: reward variables</span>
<span class="sd">            - pscore: action choice probabilities by a behavior policy</span>
<span class="sd">            - context: context vectors such as user-related features and user-item affinity scores</span>
<span class="sd">            - action_context: item-related context vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">is_timeseries_split</span><span class="p">:</span>
            <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obtain_batch_bandit_feedback</span><span class="p">(</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">is_timeseries_split</span><span class="o">=</span><span class="n">is_timeseries_split</span>
            <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_size</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">check_scalar</span><span class="p">(</span>
                <span class="n">sample_size</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sample_size&quot;</span><span class="p">,</span>
                <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
                <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">max_val</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">bootstrap_idx</span> <span class="o">=</span> <span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">key_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">]:</span>
            <span class="n">bandit_feedback</span><span class="p">[</span><span class="n">key_</span><span class="p">]</span> <span class="o">=</span> <span class="n">bandit_feedback</span><span class="p">[</span><span class="n">key_</span><span class="p">][</span><span class="n">bootstrap_idx</span><span class="p">]</span>
        <span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_size</span>
        <span class="k">return</span> <span class="n">bandit_feedback</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="ope-estimators">
<h2>OPE Estimators<a class="headerlink" href="#ope-estimators" title="Permalink to this headline">¶</a></h2>
<div class="section" id="utils">
<h3>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_array</span><span class="p">(</span>
    <span class="n">array</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">expected_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Input validation on an array.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    array: object</span>
<span class="sd">        Input object to check.</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the input array.</span>
<span class="sd">    expected_dim: int, default=1</span>
<span class="sd">        Expected dimension of the input array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">expected_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">expected_dim</span><span class="si">}</span><span class="s2">D array, but got </span><span class="si">{</span><span class="n">array</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D array&quot;</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_confidence_interval_arguments</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check confidence interval arguments.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="s2">&quot;n_bootstrap_samples&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Estimate confidence interval by nonparametric bootstrap-like procedure.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    samples: array-like</span>
<span class="sd">        Empirical observed samples to be used to estimate cumulative distribution function.</span>
<span class="sd">    alpha: float, default=0.05</span>
<span class="sd">        Significance level.</span>
<span class="sd">    n_bootstrap_samples: int, default=10000</span>
<span class="sd">        Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in bootstrap sampling.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">        Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
    <span class="p">)</span>

    <span class="n">boot_samples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_bootstrap_samples</span><span class="p">):</span>
        <span class="n">boot_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">boot_samples</span><span class="p">),</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (lower)&quot;</span><span class="p">:</span> <span class="n">lower_bound</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">% CI (upper)&quot;</span><span class="p">:</span> <span class="n">upper_bound</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_ope_inputs</span><span class="p">(</span>
    <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for ope.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">        Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    action: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">        Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># action_dist</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>

    <span class="c1"># position</span>
    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `position.shape[0] == action_dist.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;position elements must be smaller than `action_dist.shape[2]`&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;position elements must be given when `action_dist.shape[2] &gt; 1`&quot;</span>
        <span class="p">)</span>

    <span class="c1"># estimated_rewards_by_reg_model</span>
    <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>

    <span class="c1"># action, reward</span>
    <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_dist.shape[1]`&quot;</span>
            <span class="p">)</span>

    <span class="c1"># pscore</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pscore</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be 1-dimensional&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_bias_in_ope</span><span class="p">(</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">iw_hat</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">q_hat</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper to estimate a bias in OPE.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">    iw: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\pi_e(a|x)/ \\pi_b(a|x)`.</span>
<span class="sd">    iw_hat: array-like, shape (n_rounds,)</span>
<span class="sd">        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.</span>
<span class="sd">            - clipping: :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">            - switching: :math:`\\hat{w}(x,a) := w(x,a) \\cdot \\mathbb{I} \\{ w(x,a) &lt; \\lambda \\}`</span>
<span class="sd">            - shrinkage: :math:`\\hat{w}(x,a) := (\\lambda w(x,a)) / (\\lambda + w^2(x,a))`</span>
<span class="sd">        where :math:`\\lambda` is a hyperparameter value.</span>
<span class="sd">    q_hat: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.</span>
<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    estimated_bias: float</span>
<span class="sd">        Estimated the bias in OPE.</span>
<span class="sd">        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">q_hat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">)</span>
    <span class="n">estimated_bias_arr</span> <span class="o">=</span> <span class="p">(</span><span class="n">iw</span> <span class="o">-</span> <span class="n">iw_hat</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat</span><span class="p">)</span>
    <span class="n">estimated_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_bias_arr</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">estimated_bias</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="abstract-class">
<h3>Abstract Class<a class="headerlink" href="#abstract-class" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseOffPolicyEstimator</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for OPE estimators.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="direct-method">
<h3>Direct Method<a class="headerlink" href="#direct-method" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DirectMethod</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Direct Method (DM).</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    DM first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{DM}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        &amp;:= \\mathbb{E}_{\\mathcal{D}} \\left[ \\sum_{a \\in \\mathcal{A}} \\hat{q} (x_t,a) \\pi_e(a|x_t) \\right],    \\\\</span>
<span class="sd">        &amp; =  \\mathbb{E}_{\\mathcal{D}}[\\hat{q} (x_t,\\pi_e)],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`, which supports several fitting methods specific to OPE.</span>
<span class="sd">    If the regression model (:math:`\\hat{q}`) is a good approximation to the true mean reward function,</span>
<span class="sd">    this estimator accurately estimates the policy value of the evaluation policy.</span>
<span class="sd">    If the regression function fails to approximate the mean reward function well,</span>
<span class="sd">    however, the final estimator is no longer consistent.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_name: str, default=&#39;dm&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Alina Beygelzimer and John Langford.</span>
<span class="sd">    &quot;The offset tree for learning with partial labels.&quot;, 2009.</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dm&quot;</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DM estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action must be 1D array&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="doubly-robust">
<h3>Doubly Robust<a class="headerlink" href="#doubly-robust" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DoublyRobust</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Doubly Robust (DR) Estimator.</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Similar to DM, DR first learns a supervised machine learning model, such as ridge regression and gradient boosting,</span>
<span class="sd">    to estimate the mean reward function (:math:`q(x,a) = \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    It then uses it to estimate the policy value as follows.</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{DR}} (\\pi_e; \\mathcal{D}, \\hat{q})</span>
<span class="sd">        := \\mathbb{E}_{\\mathcal{D}}[\\hat{q}(x_t,\\pi_e) +  w(x_t,a_t) (r_t - \\hat{q}(x_t,a_t))],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`.</span>
<span class="sd">    :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    :math:`\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\hat{q} (x_t,\\pi):= \\mathbb{E}_{a \\sim \\pi(a|x)}[\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\pi`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>
<span class="sd">    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`,</span>
<span class="sd">    which supports several fitting methods specific to OPE such as *more robust doubly robust*.</span>
<span class="sd">    DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward</span>
<span class="sd">    function (the regression model) as a control variate to decrease the variance.</span>
<span class="sd">    It preserves the consistency of IPW if either the importance weight or</span>
<span class="sd">    the mean reward estimator is accurate (a property called double robustness).</span>
<span class="sd">    Moreover, DR is semiparametric efficient when the mean reward estimator is correctly specified.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>
<span class="sd">        DoublyRobust with a finite positive `lambda_` corresponds to Doubly Robust with Pessimistic Shrinkage of Su et al.(2020) or CAB-DR of Su et al.(2019).</span>
<span class="sd">    estimator_name: str, default=&#39;dr&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>
<span class="sd">    Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims.</span>
<span class="sd">    &quot;CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and Learning&quot;, 2019.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík.</span>
<span class="sd">    &quot;Doubly robust off-policy evaluation with shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;dr&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model or Tensor: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by the DR estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="n">q_hat_at_position</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">q_hat_factual</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
        <span class="p">]</span>
        <span class="n">pi_e_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="p">:,</span> <span class="n">position</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">estimated_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">q_hat_at_position</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">pi_e_at_position</span><span class="p">,</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;reward must be 1D array&quot;</span><span class="p">)</span>

        <span class="n">estimated_rewards</span> <span class="o">+=</span> <span class="n">iw</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">q_hat_factual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Policy value estimated by the DR estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span>
            <span class="n">array</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span><span class="p">,</span>
            <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>
<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of DR with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of DR with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
                <span class="n">delta</span><span class="o">=</span><span class="n">delta</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
                <span class="n">q_hat</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span>
                <span class="p">],</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="inverse-probability-weighting">
<h3>Inverse Probability Weighting<a class="headerlink" href="#inverse-probability-weighting" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">InverseProbabilityWeighting</span><span class="p">(</span><span class="n">BaseOffPolicyEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverse Probability Weighting (IPW) Estimator.</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Inverse Probability Weighting (IPW) estimates the policy value of evaluation policy :math:`\\pi_e` by</span>
<span class="sd">    .. math::</span>
<span class="sd">        \\hat{V}_{\\mathrm{IPW}} (\\pi_e; \\mathcal{D}) := \\mathbb{E}_{\\mathcal{D}} [ w(x_t,a_t) r_t],</span>
<span class="sd">    where :math:`\\mathcal{D}=\\{(x_t,a_t,r_t)\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by</span>
<span class="sd">    a behavior policy :math:`\\pi_b`. :math:`w(x,a):=\\pi_e (a|x)/\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.</span>
<span class="sd">    :math:`\\mathbb{E}_{\\mathcal{D}}[\\cdot]` is the empirical average over :math:`T` observations in :math:`\\mathcal{D}`.</span>
<span class="sd">    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\hat{w}(x,a) := \\min \\{ \\lambda, w(x,a) \\}`</span>
<span class="sd">    where :math:`\\lambda (&gt;0)` is a hyperparameter that decides a maximum allowed importance weight.</span>
<span class="sd">    IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight).</span>
<span class="sd">    When the behavior policy is known, IPW is unbiased and consistent for the true policy value.</span>
<span class="sd">    However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    lambda_: float, default=np.inf</span>
<span class="sd">        A maximum possible value of the importance weight.</span>
<span class="sd">        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.</span>
<span class="sd">    estimator_name: str, default=&#39;ipw&#39;.</span>
<span class="sd">        Name of the estimator.</span>
<span class="sd">    References</span>
<span class="sd">    ------------</span>
<span class="sd">    Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.</span>
<span class="sd">    &quot;Learning from Logged Implicit Exploration Data&quot;., 2010.</span>
<span class="sd">    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.</span>
<span class="sd">    &quot;Doubly Robust Policy Evaluation and Optimization.&quot;, 2014.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lambda_</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">estimator_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ipw&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lambda_&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span>
            <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;lambda_ must not be nan&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_round_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate round-wise (or sample-wise) rewards.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like or Tensor, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_rewards: array-like or Tensor, shape (n_rounds,)</span>
<span class="sd">            Rewards of each round estimated by IPW.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="c1"># weight clipping</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">iw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">iw</span>

    <span class="k">def</span> <span class="nf">estimate_policy_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        V_hat: float</span>
<span class="sd">            Estimated policy value (performance) of a given evaluation policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_interval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence interval of policy value by nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            When None is given, the effect of position on the reward will be ignored.</span>
<span class="sd">            (If only one action is chosen and there is no posion, then you can just ignore this argument.)</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=10000</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_confidence_interval: Dict[str, float]</span>
<span class="sd">            Dictionary storing the estimated mean and upper-lower confidence bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_ope_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">estimated_round_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimate_confidence_interval_by_bootstrap</span><span class="p">(</span>
            <span class="n">samples</span><span class="o">=</span><span class="n">estimated_round_rewards</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_estimate_mse_score</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias_upper_bound</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">        use_bias_upper_bound: bool, default=True</span>
<span class="sd">            Whether to use bias upper bound in hyperparameter tuning.</span>
<span class="sd">            If False, direct bias estimator is used to estimate the MSE.</span>
<span class="sd">        delta: float, default=0.05</span>
<span class="sd">            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        estimated_mse_score: float</span>
<span class="sd">            Estimated MSE score of a given clipping hyperparameter `lambda_`.</span>
<span class="sd">            MSE score is the sum of (high probability) upper bound of bias and the sample variance.</span>
<span class="sd">            This is estimated using the automatic hyperparameter tuning procedure</span>
<span class="sd">            based on Section 5 of Su et al.(2020).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># estimate the sample variance of IPW with clipping</span>
        <span class="n">sample_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">sample_variance</span> <span class="o">/=</span> <span class="n">n_rounds</span>

        <span class="c1"># estimate the (high probability) upper bound of the bias of IPW with clipping</span>
        <span class="n">iw</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span> <span class="n">action</span><span class="p">,</span> <span class="n">position</span><span class="p">]</span> <span class="o">/</span> <span class="n">pscore</span>
        <span class="k">if</span> <span class="n">use_bias_upper_bound</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_high_probability_upper_bound_bias</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span> <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_term</span> <span class="o">=</span> <span class="n">estimate_bias_in_ope</span><span class="p">(</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">iw</span><span class="o">=</span><span class="n">iw</span><span class="p">,</span>
                <span class="n">iw_hat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">iw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="n">estimated_mse_score</span> <span class="o">=</span> <span class="n">sample_variance</span> <span class="o">+</span> <span class="p">(</span><span class="n">bias_term</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimated_mse_score</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="off-policy-evaluation-class">
<h2>Off-Policy Evaluation Class<a class="headerlink" href="#off-policy-evaluation-class" title="Permalink to this headline">¶</a></h2>
<p>Off-Policy Evaluation Class to Streamline OPE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">logging</span> <span class="kn">import</span> <span class="n">getLogger</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">OffPolicyEvaluation</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Class to conduct OPE by multiple estimators simultaneously.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    bandit_feedback: BanditFeedback</span>
<span class="sd">        Logged bandit feedback data used to conduct OPE.</span>
<span class="sd">    ope_estimators: List[BaseOffPolicyEstimator]</span>
<span class="sd">        List of OPE estimators used to evaluate the policy value of evaluation policy.</span>
<span class="sd">        Estimators must follow the interface of `obp.ope.BaseOffPolicyEstimator`.</span>
<span class="sd">    Examples</span>
<span class="sd">    ----------</span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">        # a case for implementing OPE of the BernoulliTS policy</span>
<span class="sd">        # using log data generated by the Random policy</span>
<span class="sd">        &gt;&gt;&gt; from obp.dataset import OpenBanditDataset</span>
<span class="sd">        &gt;&gt;&gt; from obp.policy import BernoulliTS</span>
<span class="sd">        &gt;&gt;&gt; from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW</span>
<span class="sd">        # (1) Data loading and preprocessing</span>
<span class="sd">        &gt;&gt;&gt; dataset = OpenBanditDataset(behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;)</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback = dataset.obtain_batch_bandit_feedback()</span>
<span class="sd">        &gt;&gt;&gt; bandit_feedback.keys()</span>
<span class="sd">        dict_keys([&#39;n_rounds&#39;, &#39;n_actions&#39;, &#39;action&#39;, &#39;position&#39;, &#39;reward&#39;, &#39;pscore&#39;, &#39;context&#39;, &#39;action_context&#39;])</span>
<span class="sd">        # (2) Off-Policy Learning</span>
<span class="sd">        &gt;&gt;&gt; evaluation_policy = BernoulliTS(</span>
<span class="sd">            n_actions=dataset.n_actions,</span>
<span class="sd">            len_list=dataset.len_list,</span>
<span class="sd">            is_zozotown_prior=True, # replicate the policy in the ZOZOTOWN production</span>
<span class="sd">            campaign=&quot;all&quot;,</span>
<span class="sd">            random_state=12345</span>
<span class="sd">        )</span>
<span class="sd">        &gt;&gt;&gt; action_dist = evaluation_policy.compute_batch_action_dist(</span>
<span class="sd">            n_sim=100000, n_rounds=bandit_feedback[&quot;n_rounds&quot;]</span>
<span class="sd">        )</span>
<span class="sd">        # (3) Off-Policy Evaluation</span>
<span class="sd">        &gt;&gt;&gt; ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value</span>
<span class="sd">        {&#39;ipw&#39;: 0.004553...}</span>
<span class="sd">        # policy value improvement of BernoulliTS over the Random policy estimated by IPW</span>
<span class="sd">        &gt;&gt;&gt; estimated_policy_value_improvement = estimated_policy_value[&#39;ipw&#39;] / bandit_feedback[&#39;reward&#39;].mean()</span>
<span class="sd">        # our OPE procedure suggests that BernoulliTS improves Random by 19.81%</span>
<span class="sd">        &gt;&gt;&gt; print(estimated_policy_value_improvement)</span>
<span class="sd">        1.198126...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bandit_feedback</span><span class="p">:</span> <span class="n">BanditFeedback</span>
    <span class="n">ope_estimators</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseOffPolicyEstimator</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">key_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing key of </span><span class="si">{</span><span class="n">key_</span><span class="si">}</span><span class="s2"> in &#39;bandit_feedback&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">[</span><span class="n">estimator</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DirectMethod</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">DoublyRobust</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_create_estimator_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`&quot;&quot;&quot;</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_dist&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">check_array</span><span class="p">(</span>
                    <span class="n">array</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span>
                    <span class="n">expected_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Expected `estimated_rewards_by_reg_model[</span><span class="si">{</span><span class="n">estimator_name</span><span class="si">}</span><span class="s2">].shape == action_dist.shape`, but found it False.&quot;</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="n">estimated_rewards_by_reg_model</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{</span>
                <span class="n">input_</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="n">input_</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="s2">&quot;pscore&quot;</span><span class="p">]</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">:</span>
            <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="s2">&quot;action_dist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_dist</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                        <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="s2">&quot;estimated_rewards_by_reg_model&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_by_reg_model</span>

        <span class="k">return</span> <span class="n">estimator_inputs</span>

    <span class="k">def</span> <span class="nf">estimate_policy_values</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate the policy value of evaluation policy.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing estimated policy values by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">policy_value_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_dict</span>

    <span class="k">def</span> <span class="nf">estimate_intervals</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Estimate confidence intervals of policy values using nonparametric bootstrap procedure.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_value_interval_dict: Dict[str, Dict[str, float]]</span>
<span class="sd">            Dictionary containing confidence intervals of estimated policy value estimated</span>
<span class="sd">            using nonparametric bootstrap procedure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_model_dependent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">estimated_rewards_by_reg_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given&quot;</span>
                <span class="p">)</span>

        <span class="n">check_confidence_interval_arguments</span><span class="p">(</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">policy_value_interval_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_interval</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">],</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">policy_value_interval_dict</span>

    <span class="k">def</span> <span class="nf">summarize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Summarize policy values and their confidence intervals estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]</span>
<span class="sd">            Policy values and their confidence intervals Estimated by OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_policy_values</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;estimated_policy_value&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">policy_value_interval_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimate_intervals</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                <span class="n">n_bootstrap_samples</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">policy_value_of_behavior_policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">policy_value_df</span> <span class="o">=</span> <span class="n">policy_value_df</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">policy_value_of_behavior_policy</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Policy value of the behavior policy is </span><span class="si">{</span><span class="n">policy_value_of_behavior_policy</span><span class="si">}</span><span class="s2"> (&lt;=0); relative estimated policy value is set to np.nan&quot;</span>
            <span class="p">)</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy_value_df</span><span class="p">[</span><span class="s2">&quot;relative_estimated_policy_value&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">policy_value_df</span><span class="o">.</span><span class="n">estimated_policy_value</span> <span class="o">/</span> <span class="n">policy_value_of_behavior_policy</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">policy_value_df</span><span class="p">,</span> <span class="n">policy_value_interval_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_round_rewards_dict</span><span class="p">[</span>
                <span class="n">estimator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span><span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">])</span>
        <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">estimated_round_rewards_dict</span><span class="p">)</span>
        <span class="n">estimated_round_rewards_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
            <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">key</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">estimated_round_rewards_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()},</span>
            <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
            <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
            <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;OPE Estimators&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Evaluate estimation performance of OPE estimators.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        Evaluate the estimation performance of OPE estimators by relative estimation error (relative-EE) or squared error (SE):</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{Relative-EE} (\\hat{V}; \\mathcal{D}) = \\left|  \\frac{\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi)}{V(\\pi)} \\right|,</span>
<span class="sd">        .. math ::</span>
<span class="sd">            \\text{SE} (\\hat{V}; \\mathcal{D}) = \\left(\\hat{V}(\\pi; \\mathcal{D}) - V(\\pi) \\right)^2,</span>
<span class="sd">        where :math:`V({\\pi})` is the ground-truth policy value of the evalation policy :math:`\\pi_e` (often estimated using on-policy estimation).</span>
<span class="sd">        :math:`\\hat{V}(\\pi; \\mathcal{D})` is an estimated policy value by an OPE estimator :math:`\\hat{V}` and logged bandit feedback :math:`\\mathcal{D}`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_dict: Dict[str, float]</span>
<span class="sd">            Dictionary containing evaluation metric for evaluating the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span>
            <span class="n">ground_truth_policy_value</span><span class="p">,</span>
            <span class="s2">&quot;ground_truth_policy_value&quot;</span><span class="p">,</span>
            <span class="nb">float</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span> <span class="s2">&quot;se&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;metric must be either &#39;relative-ee&#39; or &#39;se&#39;, but </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span> <span class="ow">and</span> <span class="n">ground_truth_policy_value</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ground_truth_policy_value must be non-zero when metric is relative-ee&quot;</span>
            <span class="p">)</span>

        <span class="n">eval_metric_ope_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
            <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">estimated_policy_value</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
                <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">:</span>
                <span class="n">relative_ee_</span> <span class="o">=</span> <span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">relative_ee_</span> <span class="o">/=</span> <span class="n">ground_truth_policy_value</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">relative_ee_</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;se&quot;</span><span class="p">:</span>
                <span class="n">se_</span> <span class="o">=</span> <span class="p">(</span><span class="n">estimated_policy_value</span> <span class="o">-</span> <span class="n">ground_truth_policy_value</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">eval_metric_ope_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">se_</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_dict</span>

    <span class="k">def</span> <span class="nf">summarize_estimators_comparison</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ground_truth_policy_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relative-ee&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Summarize performance comparisons of OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ground_truth policy value: float</span>
<span class="sd">            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\pi_e)`.</span>
<span class="sd">            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        metric: str, default=&quot;relative-ee&quot;</span>
<span class="sd">            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">            Must be either &quot;relative-ee&quot; or &quot;se&quot;.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        eval_metric_ope_df: DataFrame</span>
<span class="sd">            Evaluation metric to evaluate and compare the estimation performance of OPE estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_metric_ope_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
                <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_metric_ope_df</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">visualize_off_policy_estimates_of_multiple_policies</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_name_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">action_dist_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">is_relative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fig_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;estimated_policy_value.png&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Visualize policy values estimated by OPE estimators.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        policy_name_list: List[str]</span>
<span class="sd">            List of the names of evaluation policies.</span>
<span class="sd">        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]</span>
<span class="sd">            List of action choice probabilities by the evaluation policies (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None</span>
<span class="sd">            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\hat{q}(x_t,a_t)`.</span>
<span class="sd">            When an array-like is given, all OPE estimators use it.</span>
<span class="sd">            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.</span>
<span class="sd">            When it is not given, model-dependent estimators such as DM and DR cannot be used.</span>
<span class="sd">        alpha: float, default=0.05</span>
<span class="sd">            Significance level.</span>
<span class="sd">        n_bootstrap_samples: int, default=100</span>
<span class="sd">            Number of resampling performed in the bootstrap procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            Controls the random seed in bootstrap sampling.</span>
<span class="sd">        is_relative: bool, default=False,</span>
<span class="sd">            If True, the method visualizes the estimated policy values of evaluation policy</span>
<span class="sd">            relative to the ground-truth policy value of behavior policy.</span>
<span class="sd">        fig_dir: Path, default=None</span>
<span class="sd">            Path to store the bar figure.</span>
<span class="sd">            If &#39;None&#39; is given, the figure will not be saved.</span>
<span class="sd">        fig_name: str, default=&quot;estimated_policy_value.png&quot;</span>
<span class="sd">            Name of the bar figure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;the length of policy_name_list must be the same as action_dist_list&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">fig_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_dir</span><span class="p">,</span> <span class="n">Path</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a Path&quot;</span>
        <span class="k">if</span> <span class="n">fig_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fig_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;fig_dir must be a string&quot;</span>

        <span class="n">estimated_round_rewards_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">estimator_name</span><span class="p">:</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">policy_name</span><span class="p">,</span> <span class="n">action_dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">,</span> <span class="n">action_dist_list</span><span class="p">):</span>
            <span class="n">estimator_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_estimator_inputs</span><span class="p">(</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span>
                    <span class="n">policy_name</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">_estimate_round_rewards</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">estimator_inputs</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
                <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">6.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">estimator_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ope_estimators_</span><span class="p">):</span>
            <span class="n">estimated_round_rewards_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
                <span class="n">estimated_round_rewards_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_relative</span><span class="p">:</span>
                <span class="n">estimated_round_rewards_df</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_dist_list</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
                <span class="n">data</span><span class="o">=</span><span class="n">estimated_round_rewards_df</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                <span class="n">ci</span><span class="o">=</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">),</span>
                <span class="n">n_boot</span><span class="o">=</span><span class="n">n_bootstrap_samples</span><span class="p">,</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">estimator_name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Estimated Policy Value (± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span><span class="si">}</span><span class="s2">% CI)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">policy_name_list</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fig_dir</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">fig_dir</span> <span class="o">/</span> <span class="n">fig_name</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="base-models">
<h2>Base Models<a class="headerlink" href="#base-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Utils<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_bandit_feedback_inputs</span><span class="p">(</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">expected_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ValueError</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Check inputs for bandit learning or simulation.</span>
<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">        Context vectors in each round, i.e., :math:`x_t`.</span>
<span class="sd">    action: array-like, shape (n_rounds,)</span>
<span class="sd">        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">    reward: array-like, shape (n_rounds,)</span>
<span class="sd">        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">    expected_reward: array-like, shape (n_rounds, n_actions), default=None</span>
<span class="sd">        Expected rewards (or outcome) in each round, i.e., :math:`\\mathbb{E}[r_t]`.</span>
<span class="sd">    position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">    pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">        Propensity scores, the probability of selecting each action by behavior policy,</span>
<span class="sd">        in the given logged bandit data.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context)</span>
<span class="sd">        Context vectors characterizing each action.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action elements must be non-negative integers&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">expected_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">expected_reward</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;expected_reward&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">expected_reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `expected_reward.shape[1]`&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pscore&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pscore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">pscore</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pscore must be positive&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">position</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;position&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">position</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">position</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;position elements must be non-negative integers&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected `context.shape[0] == action.shape[0] == reward.shape[0]`&quot;</span>
                <span class="s2">&quot;, but found it False&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">action_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_array</span><span class="p">(</span><span class="n">array</span><span class="o">=</span><span class="n">action_context</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;action_context&quot;</span><span class="p">,</span> <span class="n">expected_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">action_context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;action elements must be smaller than `action_context.shape[0]`&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="regression-model">
<h3>Regression Model<a class="headerlink" href="#regression-model" title="Permalink to this headline">¶</a></h3>
<p>Regression Model Class for Estimating Mean Reward Functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">RegressionModel</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Machine learning model to estimate the mean reward function (:math:`q(x,a):= \\mathbb{E}[r|x,a]`).</span>
<span class="sd">    Note</span>
<span class="sd">    -------</span>
<span class="sd">    Reward (or outcome) :math:`r` must be either binary or continuous.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ------------</span>
<span class="sd">    base_model: BaseEstimator</span>
<span class="sd">        A machine learning model used to estimate the mean reward function.</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    action_context: array-like, shape (n_actions, dim_action_context), default=None</span>
<span class="sd">        Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        If not given, one-hot encoding of the action variable is used as default.</span>
<span class="sd">    fitting_method: str, default=&#39;normal&#39;</span>
<span class="sd">        Method to fit the regression model.</span>
<span class="sd">        Must be one of [&#39;normal&#39;, &#39;iw&#39;, &#39;mrdr&#39;] where &#39;iw&#39; stands for importance weighting and</span>
<span class="sd">        &#39;mrdr&#39; stands for more robust doubly robust.</span>
<span class="sd">    References</span>
<span class="sd">    -----------</span>
<span class="sd">    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.</span>
<span class="sd">    &quot;More Robust Doubly Robust Off-policy Evaluation.&quot;, 2018.</span>
<span class="sd">    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.</span>
<span class="sd">    &quot;Doubly Robust Off-Policy Evaluation with Shrinkage.&quot;, 2020.</span>
<span class="sd">    Yusuke Narita, Shota Yasui, and Kohei Yata.</span>
<span class="sd">    &quot;Off-policy Bandit and Reinforcement Learning.&quot;, 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">base_model</span><span class="p">:</span> <span class="n">BaseEstimator</span>
    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">action_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">fitting_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;fitting_method must be one of &#39;normal&#39;, &#39;iw&#39;, or &#39;mrdr&#39;, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span><span class="si">}</span><span class="s2"> is given&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;base_model must be BaseEstimator or a child class of BaseEstimator&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,)</span>
<span class="sd">            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\pi_b(a_t|x_t)`.</span>
<span class="sd">            When None is given, behavior policy is assumed to be uniform.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;action_dist must be a probability distribution&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">position</span> <span class="o">==</span> <span class="n">position_</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No training data at position </span><span class="si">{</span><span class="n">position_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># train the base model according to the given `fitting method`</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action_dist_at_position</span> <span class="o">=</span> <span class="n">action_dist</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">),</span>
                    <span class="n">action</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span>
                <span class="p">][</span><span class="n">idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;iw&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span> <span class="o">/</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="o">==</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">action_dist_at_position</span>
                    <span class="n">sample_weight</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="n">sample_weight</span> <span class="o">/=</span> <span class="n">pscore</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                        <span class="n">X</span><span class="p">,</span> <span class="n">reward</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Predict the mean reward function.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds_of_new_data, dim_context)</span>
<span class="sd">            Context vectors of new data.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds_of_new_data, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_rounds_of_new_data</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ones_n_rounds_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">action_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">position_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_process_for_reg_model</span><span class="p">(</span>
                    <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                    <span class="n">action</span><span class="o">=</span><span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">])</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_list</span><span class="p">[</span><span class="n">position_</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_rounds_of_new_data</span><span class="p">),</span>
                    <span class="n">action_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                    <span class="n">position_</span> <span class="o">*</span> <span class="n">ones_n_rounds_arr</span><span class="p">,</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">estimated_rewards_</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">reward</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">pscore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_folds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Fit the regression model on given logged bandit feedback data and predict the reward function of the same data.</span>
<span class="sd">        Note</span>
<span class="sd">        ------</span>
<span class="sd">        When `n_folds` is larger than 1, then the cross-fitting procedure is applied.</span>
<span class="sd">        See the reference for the details about the cross-fitting technique.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        context: array-like, shape (n_rounds, dim_context)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        reward: array-like, shape (n_rounds,)</span>
<span class="sd">            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.</span>
<span class="sd">        pscore: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Action choice probabilities (propensity score) of a behavior policy</span>
<span class="sd">            in the training logged bandit feedback.</span>
<span class="sd">            When None is given, the the behavior policy is assumed to be a uniform one.</span>
<span class="sd">        position: array-like, shape (n_rounds,), default=None</span>
<span class="sd">            Position of recommendation interface where action was presented in each round of the given logged bandit data.</span>
<span class="sd">            If None is set, a regression model assumes that there is only one position.</span>
<span class="sd">            When `len_list` &gt; 1, this position argument has to be set.</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None</span>
<span class="sd">            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\pi_e(a_t|x_t)`.</span>
<span class="sd">            When either of &#39;iw&#39; or &#39;mrdr&#39; is used as the &#39;fitting_method&#39; argument, then `action_dist` must be given.</span>
<span class="sd">        n_folds: int, default=1</span>
<span class="sd">            Number of folds in the cross-fitting procedure.</span>
<span class="sd">            When 1 is given, the regression model is trained on the whole logged bandit feedback data.</span>
<span class="sd">            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.</span>
<span class="sd">        random_state: int, default=None</span>
<span class="sd">            `random_state` affects the ordering of the indices, which controls the randomness of each fold.</span>
<span class="sd">            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Expected rewards of new data estimated by the regression model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_bandit_feedback_inputs</span><span class="p">(</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
            <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
            <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
            <span class="n">action_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_rounds</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">check_scalar</span><span class="p">(</span><span class="n">n_folds</span><span class="p">,</span> <span class="s2">&quot;n_folds&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">position</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;position elements must be smaller than len_list, but the maximum value is </span><span class="si">{</span><span class="n">position</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> (&gt;= </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fitting_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;iw&quot;</span><span class="p">,</span> <span class="s2">&quot;mrdr&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;when fitting_method is either &#39;iw&#39; or &#39;mrdr&#39;, action_dist (a 3-dimensional ndarray) must be given&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;shape of action_dist must be (n_rounds, n_actions, len_list)=(</span><span class="si">{</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">action_dist</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">pscore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>

        <span class="k">if</span> <span class="n">n_folds</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">,</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">,</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">kf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
            <span class="n">action_dist_tr</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">action_dist</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">action_dist</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">action_dist</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">reward</span><span class="o">=</span><span class="n">reward</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">pscore</span><span class="o">=</span><span class="n">pscore</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">position</span><span class="o">=</span><span class="n">position</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span>
                <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist_tr</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">estimated_rewards_by_reg_model</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">estimated_rewards_by_reg_model</span>

    <span class="k">def</span> <span class="nf">_pre_process_for_reg_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">action_context</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Preprocess feature vectors to train a regression model.</span>
<span class="sd">        Note</span>
<span class="sd">        -----</span>
<span class="sd">        Please override this method if you want to use another feature enginnering</span>
<span class="sd">        for training the regression model.</span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        context: array-like, shape (n_rounds,)</span>
<span class="sd">            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.</span>
<span class="sd">        action: array-like, shape (n_rounds,)</span>
<span class="sd">            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.</span>
<span class="sd">        action_context: array-like, shape shape (n_actions, dim_action_context)</span>
<span class="sd">            Context vector characterizing action (i.e., vector representation of each action).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">action_context</span><span class="p">[</span><span class="n">action</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="policies">
<h2>Policies<a class="headerlink" href="#policies" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
  
<span class="kn">import</span> <span class="nn">enum</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>

<span class="c1"># import pkg_resources</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_scalar</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">prior_bts</span><span class="o">.</span><span class="n">yaml</span>
<span class="nb">all</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">62.0</span>
    <span class="o">-</span> <span class="mf">142.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">857.0</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">100.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">71.0</span>
    <span class="o">-</span> <span class="mf">61.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">16.0</span>
    <span class="o">-</span> <span class="mf">518.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">121.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">204.0</span>
    <span class="o">-</span> <span class="mf">58.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">19.0</span>
    <span class="o">-</span> <span class="mf">42.0</span>
    <span class="o">-</span> <span class="mf">1013.0</span>
    <span class="o">-</span> <span class="mf">2.0</span>
    <span class="o">-</span> <span class="mf">328.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">31.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">138.0</span>
    <span class="o">-</span> <span class="mf">45.0</span>
    <span class="o">-</span> <span class="mf">55.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">38.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">401.0</span>
    <span class="o">-</span> <span class="mf">52.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">35.0</span>
    <span class="o">-</span> <span class="mf">133.0</span>
    <span class="o">-</span> <span class="mf">52.0</span>
    <span class="o">-</span> <span class="mf">820.0</span>
    <span class="o">-</span> <span class="mf">43.0</span>
    <span class="o">-</span> <span class="mf">195.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">42.0</span>
    <span class="o">-</span> <span class="mf">40.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">54.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">65.0</span>
    <span class="o">-</span> <span class="mf">246.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12198.0</span>
    <span class="o">-</span> <span class="mf">3566.0</span>
    <span class="o">-</span> <span class="mf">15993.0</span>
    <span class="o">-</span> <span class="mf">35522.0</span>
    <span class="o">-</span> <span class="mf">2367.0</span>
    <span class="o">-</span> <span class="mf">4609.0</span>
    <span class="o">-</span> <span class="mf">3171.0</span>
    <span class="o">-</span> <span class="mf">181745.0</span>
    <span class="o">-</span> <span class="mf">4372.0</span>
    <span class="o">-</span> <span class="mf">4951.0</span>
    <span class="o">-</span> <span class="mf">3100.0</span>
    <span class="o">-</span> <span class="mf">24665.0</span>
    <span class="o">-</span> <span class="mf">13210.0</span>
    <span class="o">-</span> <span class="mf">7061.0</span>
    <span class="o">-</span> <span class="mf">18061.0</span>
    <span class="o">-</span> <span class="mf">17449.0</span>
    <span class="o">-</span> <span class="mf">5644.0</span>
    <span class="o">-</span> <span class="mf">6787.0</span>
    <span class="o">-</span> <span class="mf">111326.0</span>
    <span class="o">-</span> <span class="mf">8776.0</span>
    <span class="o">-</span> <span class="mf">3334.0</span>
    <span class="o">-</span> <span class="mf">2271.0</span>
    <span class="o">-</span> <span class="mf">7389.0</span>
    <span class="o">-</span> <span class="mf">2659.0</span>
    <span class="o">-</span> <span class="mf">3665.0</span>
    <span class="o">-</span> <span class="mf">4724.0</span>
    <span class="o">-</span> <span class="mf">3561.0</span>
    <span class="o">-</span> <span class="mf">5085.0</span>
    <span class="o">-</span> <span class="mf">27407.0</span>
    <span class="o">-</span> <span class="mf">4601.0</span>
    <span class="o">-</span> <span class="mf">4756.0</span>
    <span class="o">-</span> <span class="mf">4120.0</span>
    <span class="o">-</span> <span class="mf">4736.0</span>
    <span class="o">-</span> <span class="mf">3788.0</span>
    <span class="o">-</span> <span class="mf">45292.0</span>
    <span class="o">-</span> <span class="mf">14719.0</span>
    <span class="o">-</span> <span class="mf">2189.0</span>
    <span class="o">-</span> <span class="mf">5589.0</span>
    <span class="o">-</span> <span class="mf">11995.0</span>
    <span class="o">-</span> <span class="mf">222255.0</span>
    <span class="o">-</span> <span class="mf">2308.0</span>
    <span class="o">-</span> <span class="mf">70034.0</span>
    <span class="o">-</span> <span class="mf">4801.0</span>
    <span class="o">-</span> <span class="mf">8274.0</span>
    <span class="o">-</span> <span class="mf">5421.0</span>
    <span class="o">-</span> <span class="mf">31912.0</span>
    <span class="o">-</span> <span class="mf">12213.0</span>
    <span class="o">-</span> <span class="mf">13576.0</span>
    <span class="o">-</span> <span class="mf">6230.0</span>
    <span class="o">-</span> <span class="mf">10382.0</span>
    <span class="o">-</span> <span class="mf">4141.0</span>
    <span class="o">-</span> <span class="mf">85731.0</span>
    <span class="o">-</span> <span class="mf">12811.0</span>
    <span class="o">-</span> <span class="mf">2707.0</span>
    <span class="o">-</span> <span class="mf">2250.0</span>
    <span class="o">-</span> <span class="mf">2668.0</span>
    <span class="o">-</span> <span class="mf">2886.0</span>
    <span class="o">-</span> <span class="mf">9581.0</span>
    <span class="o">-</span> <span class="mf">9465.0</span>
    <span class="o">-</span> <span class="mf">28336.0</span>
    <span class="o">-</span> <span class="mf">12062.0</span>
    <span class="o">-</span> <span class="mf">162793.0</span>
    <span class="o">-</span> <span class="mf">12107.0</span>
    <span class="o">-</span> <span class="mf">41240.0</span>
    <span class="o">-</span> <span class="mf">3162.0</span>
    <span class="o">-</span> <span class="mf">11604.0</span>
    <span class="o">-</span> <span class="mf">10818.0</span>
    <span class="o">-</span> <span class="mf">2923.0</span>
    <span class="o">-</span> <span class="mf">8897.0</span>
    <span class="o">-</span> <span class="mf">8654.0</span>
    <span class="o">-</span> <span class="mf">4000.0</span>
    <span class="o">-</span> <span class="mf">6580.0</span>
    <span class="o">-</span> <span class="mf">3174.0</span>
    <span class="o">-</span> <span class="mf">6766.0</span>
    <span class="o">-</span> <span class="mf">2602.0</span>
    <span class="o">-</span> <span class="mf">14506.0</span>
    <span class="o">-</span> <span class="mf">3968.0</span>
    <span class="o">-</span> <span class="mf">7523.0</span>
    <span class="o">-</span> <span class="mf">16532.0</span>
    <span class="o">-</span> <span class="mf">51964.0</span>
<span class="n">men</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">62.0</span>
    <span class="o">-</span> <span class="mf">142.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">100.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">71.0</span>
    <span class="o">-</span> <span class="mf">61.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">16.0</span>
    <span class="o">-</span> <span class="mf">518.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">8.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">121.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">4.0</span>
    <span class="o">-</span> <span class="mf">32.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">22.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">23.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">54.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12198.0</span>
    <span class="o">-</span> <span class="mf">3566.0</span>
    <span class="o">-</span> <span class="mf">15993.0</span>
    <span class="o">-</span> <span class="mf">35522.0</span>
    <span class="o">-</span> <span class="mf">2367.0</span>
    <span class="o">-</span> <span class="mf">3100.0</span>
    <span class="o">-</span> <span class="mf">24665.0</span>
    <span class="o">-</span> <span class="mf">13210.0</span>
    <span class="o">-</span> <span class="mf">7061.0</span>
    <span class="o">-</span> <span class="mf">18061.0</span>
    <span class="o">-</span> <span class="mf">17449.0</span>
    <span class="o">-</span> <span class="mf">5644.0</span>
    <span class="o">-</span> <span class="mf">6787.0</span>
    <span class="o">-</span> <span class="mf">111326.0</span>
    <span class="o">-</span> <span class="mf">8776.0</span>
    <span class="o">-</span> <span class="mf">3334.0</span>
    <span class="o">-</span> <span class="mf">2271.0</span>
    <span class="o">-</span> <span class="mf">7389.0</span>
    <span class="o">-</span> <span class="mf">2659.0</span>
    <span class="o">-</span> <span class="mf">3665.0</span>
    <span class="o">-</span> <span class="mf">4724.0</span>
    <span class="o">-</span> <span class="mf">3561.0</span>
    <span class="o">-</span> <span class="mf">5085.0</span>
    <span class="o">-</span> <span class="mf">27407.0</span>
    <span class="o">-</span> <span class="mf">4601.0</span>
    <span class="o">-</span> <span class="mf">2923.0</span>
    <span class="o">-</span> <span class="mf">8897.0</span>
    <span class="o">-</span> <span class="mf">8654.0</span>
    <span class="o">-</span> <span class="mf">4000.0</span>
    <span class="o">-</span> <span class="mf">6580.0</span>
    <span class="o">-</span> <span class="mf">3174.0</span>
    <span class="o">-</span> <span class="mf">6766.0</span>
    <span class="o">-</span> <span class="mf">2602.0</span>
    <span class="o">-</span> <span class="mf">14506.0</span>
<span class="n">women</span><span class="p">:</span>
  <span class="n">alpha</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">984.0</span>
    <span class="o">-</span> <span class="mf">13.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">15.0</span>
    <span class="o">-</span> <span class="mf">11.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">9.0</span>
    <span class="o">-</span> <span class="mf">200.0</span>
    <span class="o">-</span> <span class="mf">72.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">49.0</span>
    <span class="o">-</span> <span class="mf">1278.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">325.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">27.0</span>
    <span class="o">-</span> <span class="mf">14.0</span>
    <span class="o">-</span> <span class="mf">169.0</span>
    <span class="o">-</span> <span class="mf">48.0</span>
    <span class="o">-</span> <span class="mf">47.0</span>
    <span class="o">-</span> <span class="mf">18.0</span>
    <span class="o">-</span> <span class="mf">40.0</span>
    <span class="o">-</span> <span class="mf">12.0</span>
    <span class="o">-</span> <span class="mf">447.0</span>
    <span class="o">-</span> <span class="mf">46.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">3.0</span>
    <span class="o">-</span> <span class="mf">5.0</span>
    <span class="o">-</span> <span class="mf">7.0</span>
    <span class="o">-</span> <span class="mf">35.0</span>
    <span class="o">-</span> <span class="mf">34.0</span>
    <span class="o">-</span> <span class="mf">99.0</span>
    <span class="o">-</span> <span class="mf">30.0</span>
    <span class="o">-</span> <span class="mf">880.0</span>
    <span class="o">-</span> <span class="mf">51.0</span>
    <span class="o">-</span> <span class="mf">182.0</span>
    <span class="o">-</span> <span class="mf">6.0</span>
    <span class="o">-</span> <span class="mf">45.0</span>
    <span class="o">-</span> <span class="mf">39.0</span>
    <span class="o">-</span> <span class="mf">10.0</span>
    <span class="o">-</span> <span class="mf">24.0</span>
    <span class="o">-</span> <span class="mf">72.0</span>
    <span class="o">-</span> <span class="mf">229.0</span>
  <span class="n">beta</span><span class="p">:</span>
    <span class="o">-</span> <span class="mf">3612.0</span>
    <span class="o">-</span> <span class="mf">3173.0</span>
    <span class="o">-</span> <span class="mf">204484.0</span>
    <span class="o">-</span> <span class="mf">4517.0</span>
    <span class="o">-</span> <span class="mf">4765.0</span>
    <span class="o">-</span> <span class="mf">5331.0</span>
    <span class="o">-</span> <span class="mf">4131.0</span>
    <span class="o">-</span> <span class="mf">4728.0</span>
    <span class="o">-</span> <span class="mf">4028.0</span>
    <span class="o">-</span> <span class="mf">44280.0</span>
    <span class="o">-</span> <span class="mf">17918.0</span>
    <span class="o">-</span> <span class="mf">2309.0</span>
    <span class="o">-</span> <span class="mf">4339.0</span>
    <span class="o">-</span> <span class="mf">12922.0</span>
    <span class="o">-</span> <span class="mf">270771.0</span>
    <span class="o">-</span> <span class="mf">2480.0</span>
    <span class="o">-</span> <span class="mf">68475.0</span>
    <span class="o">-</span> <span class="mf">5129.0</span>
    <span class="o">-</span> <span class="mf">7367.0</span>
    <span class="o">-</span> <span class="mf">5819.0</span>
    <span class="o">-</span> <span class="mf">38026.0</span>
    <span class="o">-</span> <span class="mf">13047.0</span>
    <span class="o">-</span> <span class="mf">11604.0</span>
    <span class="o">-</span> <span class="mf">5394.0</span>
    <span class="o">-</span> <span class="mf">10912.0</span>
    <span class="o">-</span> <span class="mf">4439.0</span>
    <span class="o">-</span> <span class="mf">94485.0</span>
    <span class="o">-</span> <span class="mf">10700.0</span>
    <span class="o">-</span> <span class="mf">2679.0</span>
    <span class="o">-</span> <span class="mf">2319.0</span>
    <span class="o">-</span> <span class="mf">2578.0</span>
    <span class="o">-</span> <span class="mf">3288.0</span>
    <span class="o">-</span> <span class="mf">9566.0</span>
    <span class="o">-</span> <span class="mf">9775.0</span>
    <span class="o">-</span> <span class="mf">20120.0</span>
    <span class="o">-</span> <span class="mf">7317.0</span>
    <span class="o">-</span> <span class="mf">172026.0</span>
    <span class="o">-</span> <span class="mf">13673.0</span>
    <span class="o">-</span> <span class="mf">37329.0</span>
    <span class="o">-</span> <span class="mf">3365.0</span>
    <span class="o">-</span> <span class="mf">10911.0</span>
    <span class="o">-</span> <span class="mf">10734.0</span>
    <span class="o">-</span> <span class="mf">4278.0</span>
    <span class="o">-</span> <span class="mf">7574.0</span>
    <span class="o">-</span> <span class="mf">16826.0</span>
    <span class="o">-</span> <span class="mf">47462.0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing prior_bts.yaml
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># configurations to replicate the Bernoulli Thompson Sampling policy used in ZOZOTOWN production</span>
<span class="n">prior_bts_file</span> <span class="o">=</span> <span class="s2">&quot;prior_bts.yaml&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prior_bts_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">production_prior_for_bts</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Policy type.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    CONTEXT_FREE:</span>
<span class="sd">        The policy type is contextfree.</span>
<span class="sd">    CONTEXTUAL:</span>
<span class="sd">        The policy type is contextual.</span>
<span class="sd">    OFFLINE:</span>
<span class="sd">        The policy type is offline.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">CONTEXT_FREE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">CONTEXTUAL</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
    <span class="n">OFFLINE</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="base-context-free-policy">
<h3>Base Context Free Policy<a class="headerlink" href="#base-context-free-policy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseContextFreePolicy</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for context-free bandit policies.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">len_list</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;n_actions&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="s2">&quot;len_list&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">policy_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PolicyType</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Type of the bandit policy.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">PolicyType</span><span class="o">.</span><span class="n">CONTEXT_FREE</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="epsilon-greedy">
<h3>Epsilon Greedy<a class="headerlink" href="#epsilon-greedy" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">EpsilonGreedy</span><span class="p">(</span><span class="n">BaseContextFreePolicy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Epsilon Greedy policy.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    epsilon: float, default=1.</span>
<span class="sd">        Exploration hyperparameter that must take value in the range of [0., 1.].</span>
<span class="sd">    policy_name: str, default=f&#39;egreedy_{epsilon}&#39;.</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize Class.&quot;&quot;&quot;</span>
        <span class="n">check_scalar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;egreedy_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        selected_actions: array-like, shape (len_list, )</span>
<span class="sd">            List of selected actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">predicted_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span>
            <span class="k">return</span> <span class="n">predicted_rewards</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action: int</span>
<span class="sd">            Selected action by the policy.</span>
<span class="sd">        reward: float</span>
<span class="sd">            Observed reward for the chosen action and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="random">
<h3>Random<a class="headerlink" href="#random" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Random</span><span class="p">(</span><span class="n">EpsilonGreedy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Random policy</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    epsilon: float, default=1.</span>
<span class="sd">        Exploration hyperparameter that must take value in the range of [0., 1.].</span>
<span class="sd">    policy_name: str, default=&#39;random&#39;.</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">policy_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>

    <span class="k">def</span> <span class="nf">compute_batch_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Compute the distribution over actions by Monte Carlo simulation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_rounds: int, default=1</span>
<span class="sd">            Number of rounds in the distribution over actions.</span>
<span class="sd">            (the size of the first axis of `action_dist`)</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Probability estimates of each arm being the best one for each sample, action, and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_rounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">action_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="bernoullits">
<h3>BernoulliTS<a class="headerlink" href="#bernoullits" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BernoulliTS</span><span class="p">(</span><span class="n">BaseContextFreePolicy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bernoulli Thompson Sampling Policy</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_actions: int</span>
<span class="sd">        Number of actions.</span>
<span class="sd">    len_list: int, default=1</span>
<span class="sd">        Length of a list of actions recommended in each impression.</span>
<span class="sd">        When Open Bandit Dataset is used, 3 should be set.</span>
<span class="sd">    batch_size: int, default=1</span>
<span class="sd">        Number of samples used in a batch parameter update.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        Controls the random seed in sampling actions.</span>
<span class="sd">    alpha: array-like, shape (n_actions, ), default=None</span>
<span class="sd">        Prior parameter vector for Beta distributions.</span>
<span class="sd">    beta: array-like, shape (n_actions, ), default=None</span>
<span class="sd">        Prior parameter vector for Beta distributions.</span>
<span class="sd">    is_zozotown_prior: bool, default=False</span>
<span class="sd">        Whether to use hyperparameters for the beta distribution used</span>
<span class="sd">        at the start of the data collection period in ZOZOTOWN.</span>
<span class="sd">    campaign: str, default=None</span>
<span class="sd">        One of the three possible campaigns considered in ZOZOTOWN, &quot;all&quot;, &quot;men&quot;, and &quot;women&quot;.</span>
<span class="sd">    policy_name: str, default=&#39;bts&#39;</span>
<span class="sd">        Name of bandit policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">alpha</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">beta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">is_zozotown_prior</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">campaign</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">policy_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bts&quot;</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialize class.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_zozotown_prior</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">campaign</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;`campaign` must be specified when `is_zozotown_prior` is True.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">production_prior_for_bts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="p">][</span><span class="s2">&quot;alpha&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">production_prior_for_bts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">campaign</span><span class="p">][</span><span class="s2">&quot;beta&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

    <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Select a list of actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        selected_actions: array-like, shape (len_list, )</span>
<span class="sd">            List of selected actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predicted_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">b</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">predicted_rewards</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update policy parameters.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        action: int</span>
<span class="sd">            Selected action by the policy.</span>
<span class="sd">        reward: float</span>
<span class="sd">            Observed reward for the chosen action and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trial</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_counts_temp</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_counts_temp</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_batch_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_rounds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">n_sim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Compute the distribution over actions by Monte Carlo simulation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_rounds: int, default=1</span>
<span class="sd">            Number of rounds in the distribution over actions.</span>
<span class="sd">            (the size of the first axis of `action_dist`)</span>
<span class="sd">        n_sim: int, default=100000</span>
<span class="sd">            Number of simulations in the Monte Carlo simulation to compute the distribution over actions.</span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        action_dist: array-like, shape (n_rounds, n_actions, len_list)</span>
<span class="sd">            Probability estimates of each arm being the best one for each sample, action, and position.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">selected_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select_action</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len_list</span><span class="p">):</span>
                <span class="n">action_count</span><span class="p">[</span><span class="n">selected_actions</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">action_count</span> <span class="o">/</span> <span class="n">n_sim</span><span class="p">,</span>
            <span class="p">(</span><span class="n">n_rounds</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">action_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<p>Policies</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluation_policy_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">bts</span><span class="o">=</span><span class="n">BernoulliTS</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">Random</span><span class="p">)</span>
<span class="n">evaluation_policy_dict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bts&#39;: __main__.BernoulliTS, &#39;random&#39;: __main__.Random}
</pre></div>
</div>
</div>
</div>
<p>Base models</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">lightgbm</span><span class="p">:</span>
    <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.01</span>
    <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">logistic_regression</span><span class="p">:</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="mi">10000</span>
    <span class="n">C</span><span class="p">:</span> <span class="mi">100</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
<span class="n">random_forest</span><span class="p">:</span>
    <span class="n">n_estimators</span><span class="p">:</span> <span class="mi">30</span>
    <span class="n">max_depth</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">min_samples_leaf</span><span class="p">:</span> <span class="mi">10</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="mi">12345</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Overwriting hyperparams.yaml
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># hyperparameters of the regression model used in model dependent OPE estimators</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;hyperparams.yaml&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">hyperparams</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">hyperparams</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lightgbm&#39;: {&#39;learning_rate&#39;: 0.01,
  &#39;max_depth&#39;: 5,
  &#39;min_samples_leaf&#39;: 10,
  &#39;n_estimators&#39;: 30,
  &#39;random_state&#39;: 12345},
 &#39;logistic_regression&#39;: {&#39;C&#39;: 100, &#39;max_iter&#39;: 10000, &#39;random_state&#39;: 12345},
 &#39;random_forest&#39;: {&#39;max_depth&#39;: 5,
  &#39;min_samples_leaf&#39;: 10,
  &#39;n_estimators&#39;: 30,
  &#39;random_state&#39;: 12345}}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">base_model_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">logistic_regression</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">,</span>
    <span class="n">lightgbm</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">,</span>
    <span class="n">random_forest</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">base_model_dict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;lightgbm&#39;: sklearn.ensemble._gb.GradientBoostingClassifier,
 &#39;logistic_regression&#39;: sklearn.linear_model._logistic.LogisticRegression,
 &#39;random_forest&#39;: sklearn.ensemble._forest.RandomForestClassifier}
</pre></div>
</div>
</div>
</div>
<p>OPE estimators</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ope_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="n">DirectMethod</span><span class="p">(),</span> <span class="n">InverseProbabilityWeighting</span><span class="p">(),</span> <span class="n">DoublyRobust</span><span class="p">()]</span>
<span class="n">ope_estimators</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[DirectMethod(estimator_name=&#39;dm&#39;),
 InverseProbabilityWeighting(lambda_=inf, estimator_name=&#39;ipw&#39;),
 DoublyRobust(lambda_=inf, estimator_name=&#39;dr&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;evaluate off-policy estimators.&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_runs&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of bootstrap sampling in the experiment.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--evaluation_policy&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bts&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;bts&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;evaluation policy, bts or random.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--base_model&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logistic_regression&quot;</span><span class="p">,</span> <span class="s2">&quot;lightgbm&quot;</span><span class="p">,</span> <span class="s2">&quot;random_forest&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;lightgbm&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;base ML model for regression model, logistic_regression, random_forest or lightgbm.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--behavior_policy&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bts&quot;</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;behavior policy, bts or random.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--campaign&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;men&quot;</span><span class="p">,</span> <span class="s2">&quot;women&quot;</span><span class="p">],</span>
    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;campaign name, men, women, or all.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_sim_to_compute_action_dist&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of monte carlo simulation to compute the action distribution of bts.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s2">&quot;--n_jobs&quot;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the maximum number of concurrently running jobs.&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--random_state&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12345</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Namespace(base_model=&#39;lightgbm&#39;, behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;, evaluation_policy=&#39;bts&#39;, n_jobs=1, n_runs=1, n_sim_to_compute_action_dist=1000000, random_state=12345)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># configurations</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_runs</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">base_model</span>
<span class="n">evaluation_policy</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">evaluation_policy</span>
<span class="n">behavior_policy</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">behavior_policy</span>
<span class="n">campaign</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">campaign</span>
<span class="n">n_sim_to_compute_action_dist</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_sim_to_compute_action_dist</span>
<span class="n">n_jobs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_jobs</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">random_state</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!git clone https://github.com/st-tech/zr-obp.git
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cloning into &#39;zr-obp&#39;...
remote: Enumerating objects: 4993, done.
remote: Counting objects: 100% (2007/2007), done.
remote: Compressing objects: 100% (860/860), done.
remote: Total 4993 (delta 1404), reused 1661 (delta 1135), pack-reused 2986
Receiving objects: 100% (4993/4993), 27.54 MiB | 29.23 MiB/s, done.
Resolving deltas: 100% (3306/3306), done.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">obd</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="p">(</span><span class="n">behavior_policy</span><span class="o">=</span><span class="n">behavior_policy</span><span class="p">,</span> <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s1">&#39;/content/zr-obp/obd&#39;</span><span class="p">))</span>
<span class="n">obd</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OpenBanditDataset(behavior_policy=&#39;random&#39;, campaign=&#39;all&#39;, data_path=PosixPath(&#39;/content/zr-obp/obd/random/all&#39;), dataset_name=&#39;obd&#39;)
</pre></div>
</div>
</div>
</div>
<p>Compute action distribution by evaluation policy</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_actions</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">len_list</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">evaluation_policy</span> <span class="o">==</span> <span class="s2">&quot;bts&quot;</span><span class="p">:</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;is_zozotown_prior&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;campaign&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">campaign</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">evaluation_policy_dict</span><span class="p">[</span><span class="n">evaluation_policy</span><span class="p">](</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">action_dist_single_round</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_batch_action_dist</span><span class="p">(</span>
    <span class="n">n_sim</span><span class="o">=</span><span class="n">n_sim_to_compute_action_dist</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ground-truth policy value of an evaluation policy, which is estimated with factual (observed) rewards (on-policy estimation)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth_policy_value</span> <span class="o">=</span> <span class="n">OpenBanditDataset</span><span class="o">.</span><span class="n">calc_on_policy_policy_value_estimate</span><span class="p">(</span>
    <span class="n">behavior_policy</span><span class="o">=</span><span class="n">evaluation_policy</span><span class="p">,</span>
    <span class="n">campaign</span><span class="o">=</span><span class="n">campaign</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="c1"># sample bootstrap from batch logged bandit feedback</span>
    <span class="n">bandit_feedback</span> <span class="o">=</span> <span class="n">obd</span><span class="o">.</span><span class="n">sample_bootstrap_bandit_feedback</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
    <span class="c1"># estimate the mean reward function with an ML model</span>
    <span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span>
        <span class="n">n_actions</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
        <span class="n">len_list</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">len_list</span><span class="p">,</span>
        <span class="n">action_context</span><span class="o">=</span><span class="n">obd</span><span class="o">.</span><span class="n">action_context</span><span class="p">,</span>
        <span class="n">base_model</span><span class="o">=</span><span class="n">base_model_dict</span><span class="p">[</span><span class="n">base_model</span><span class="p">](</span><span class="o">**</span><span class="n">hyperparams</span><span class="p">[</span><span class="n">base_model</span><span class="p">]),</span>
    <span class="p">)</span>
    <span class="n">estimated_rewards_by_reg_model</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="n">action</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">],</span>
        <span class="n">position</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
        <span class="n">pscore</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;pscore&quot;</span><span class="p">],</span>
        <span class="n">n_folds</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># 3-fold cross-fitting</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># evaluate estimators&#39; performances using relative estimation error (relative-ee)</span>
    <span class="n">ope</span> <span class="o">=</span> <span class="n">OffPolicyEvaluation</span><span class="p">(</span>
        <span class="n">bandit_feedback</span><span class="o">=</span><span class="n">bandit_feedback</span><span class="p">,</span>
        <span class="n">ope_estimators</span><span class="o">=</span><span class="n">ope_estimators</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">action_dist_single_round</span><span class="p">,</span> <span class="p">(</span><span class="n">bandit_feedback</span><span class="p">[</span><span class="s2">&quot;n_rounds&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">relative_ee_b</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">evaluate_performance_of_estimators</span><span class="p">(</span>
        <span class="n">ground_truth_policy_value</span><span class="o">=</span><span class="n">ground_truth_policy_value</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span>
        <span class="n">estimated_rewards_by_reg_model</span><span class="o">=</span><span class="n">estimated_rewards_by_reg_model</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">relative_ee_b</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processed</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">50</span><span class="p">)([</span><span class="n">delayed</span><span class="p">(</span><span class="n">process</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_runs</span><span class="p">)])</span>

<span class="n">relative_ee_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">est</span><span class="o">.</span><span class="n">estimator_name</span><span class="p">:</span> <span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="n">ope_estimators</span><span class="p">}</span>

<span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">relative_ee_b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">processed</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">estimator_name</span><span class="p">,</span> <span class="n">relative_ee_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">relative_ee_b</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">relative_ee_dict</span><span class="p">[</span><span class="n">estimator_name</span><span class="p">][</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">relative_ee_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">relative_ee_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">relative_ee_dict</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;random_state=</span><span class="si">{</span><span class="n">random_state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">relative_ee_df</span><span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================
random_state=12345
------------------------------
         mean  std
dm   0.034354  NaN
ipw  0.100573  NaN
dr   0.096567  NaN
==============================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># save results of the evaluation of off-policy estimators in &#39;./logs&#39; directory.</span>
<span class="n">log_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./logs&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="n">behavior_policy</span> <span class="o">/</span> <span class="n">campaign</span>
<span class="n">log_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">relative_ee_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">log_path</span> <span class="o">/</span> <span class="s2">&quot;relative_ee_of_ope_estimators.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "RecoHut-Projects/ope-rec",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="US773842_Off_Policy_Evaluation.html">
   Off-Policy Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L978189_Counterfactual_Policy_Evaluation.html">
   Counterfactual policy evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L845850_Evaluating_non_stationary_policies.html">
   Evaluating non-stationary policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L036590_Online_vs_Offline_Evaluation.html">
   Online vs Offline Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L410896_Open_Bandit_Dataset.html">
   Open Bandit Dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L910228_OBP_Library.html">
   OBP Library
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="C862189_Direct_method.html">
   Direct method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C674274_Inverse_Propensity_Weighting.html">
   Inverse Propensity Weighting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C235655_Doubly_Robust.html">
   Doubly Robust
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T601959_Evaluating_a_New_Fraud_Policy_with_DM%2C_IPW%2C_and_DR_Methods.html">
   Evaluating a New Fraud Policy with DM, IPW, and DR Methods
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Batch Learning from Bandit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="US773842_Off_Policy_Evaluation.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="" title=""><span class="show-for-sr"></span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                        <li data-tippy-content="Download PDF" onClick="window.print()"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/RecoHut-Projects/ope-rec/tree/main/docs/T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.ipynb" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="" data-urlpath="" data-branch=>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset";
                const repoURL = "";
                const urlPath = "";
                const branch = ""
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
    
  </body>
</html>