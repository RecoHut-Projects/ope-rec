
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluating a New Fraud Policy with DM, IPW, and DR Methods &#8212; ope-rec</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/quantecon-book-theme.1ef59f8f4e91ec8319176e8479c6af4e.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Sparsh A." />
<meta name="keywords" content="" />
<meta name="description" content=Evaluating a New Fraud Policy with DM, IPW, and DR Methods  IPS  from typing import Callable, Dict, List import pandas as pd import statistics   P95_Z_SCORE = 1 />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Evaluating a New Fraud Policy with DM, IPW, and DR Methods"/>
<meta name="twitter:description" content="Evaluating a New Fraud Policy with DM, IPW, and DR Methods  IPS  from typing import Callable, Dict, List import pandas as pd import statistics   P95_Z_SCORE = 1">
<meta name="twitter:creator" content="@">
<meta name="twitter:image" content="">

<!-- Opengraph tags -->
<meta property="og:title" content="Evaluating a New Fraud Policy with DM, IPW, and DR Methods" />
<meta property="og:type" content="website" />
<meta property="og:url" content="None" />
<meta property="og:image" content="" />
<meta property="og:description" content="Evaluating a New Fraud Policy with DM, IPW, and DR Methods  IPS  from typing import Callable, Dict, List import pandas as pd import statistics   P95_Z_SCORE = 1" />
<meta property="og:site_name" content="ope-rec" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=T601959_Evaluating_a_New_Fraud_Policy_with_DM_IPW_and_DR_Methods>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ips">
   IPS
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scenario">
     Scenario
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dm">
   DM
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Scenario
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dr">
   DR
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Scenario
    </a>
   </li>
  </ul>
 </li>
</ul>

                            <p class="logo">
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="US773842_Off_Policy_Evaluation.html">ope-rec</a></p>

                        <p class="qe-page__header-subheading">Evaluating a New Fraud Policy with DM, IPW, and DR Methods</p>

                    </div>

                    <p class="qe-page__header-authors">Sparsh A.</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <div class="tex2jax_ignore mathjax_ignore section" id="evaluating-a-new-fraud-policy-with-dm-ipw-and-dr-methods">
<h1>Evaluating a New Fraud Policy with DM, IPW, and DR Methods<a class="headerlink" href="#evaluating-a-new-fraud-policy-with-dm-ipw-and-dr-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="ips">
<h2>IPS<a class="headerlink" href="#ips" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statistics</span>


<span class="n">P95_Z_SCORE</span> <span class="o">=</span> <span class="mf">1.96</span>


<span class="k">def</span> <span class="nf">compute_list_stats</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute mean and P95 CI of mean for a list of floats.&quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">stdev</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">ci_low</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span> <span class="o">-</span> <span class="n">P95_Z_SCORE</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">std_dev</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">ci_high</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">P95_Z_SCORE</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">std_dev</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;ci_low&quot;</span><span class="p">:</span> <span class="n">ci_low</span><span class="p">,</span> <span class="s2">&quot;ci_high&quot;</span><span class="p">:</span> <span class="n">ci_high</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="p">(</span>
            <span class="n">action_probabilities</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]]</span> <span class="o">/</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_prob&quot;</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">*</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="scenario">
<h3>Scenario<a class="headerlink" href="#scenario" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>    
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use IPS to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>IPS requires that we know P(action | context) for the new policy. We can easily describe our new policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We can now get the probability that the new policy takes the same action that was taken in the production logs above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span><span class="p">[</span><span class="s2">&quot;new_action_prob&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">action_probabilities</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]],</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
      <th>new_action_prob</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
      <td>0.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
      <td>0.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
      <td>0.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We see that the new policy lets through a fraud example (row: 3) at a much higher probability. This should make the new model get penalized in offline evaluation. We also see that for row: 0, the new model has a 90% chance of allowing the transaction, but we don’t have the counterfactual knowledge of whether or not this would have been a non-fraud transaction since in production this transaction was blocked. This demonstrates one of the drawbacks of offline policy evaluation, but with more data we’d ideally see a different action taken in the same situation (due to exploration).</p>
<p>Now we will score the new model using IPS:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 17.59,
  &#39;ci_low&#39;: -11.19,
  &#39;mean&#39;: 3.2},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 46.68,
  &#39;ci_low&#39;: -109.88,
  &#39;mean&#39;: -31.6}}
</pre></div>
</div>
</div>
</div>
<p>The expected reward per observation for the new policy is much worse than the logging policy (due to the observation that allowed fraud to go through (row: 3)) so we wouldn’t roll out this new policy into an A/B test or production and instead should test some different policies offline.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
<div class="section" id="dm">
<h2>DM<a class="headerlink" href="#dm" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NoReturn</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>


<span class="k">def</span> <span class="nf">fit_gbdt_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT regressor.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingRegressor</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_gbdt_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge regression.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">Predictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_preprocess_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="c1"># preprocess context</span>
        <span class="n">context_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_column_order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">context_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># preprocess actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possible_actions</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">action_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">one_hot_action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>

        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">one_hot_action_values</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">values</span>

        <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_data</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">fit_gbdt_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_stats</span> <span class="o">=</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="c1"># train a model that predicts reward given (context, action)</span>
    <span class="n">reward_model</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
    <span class="n">reward_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">evaluate_raw</span><span class="p">(</span>
                <span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span>
            <span class="p">)</span>
        <span class="p">]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="p">:</span> <span class="n">Predictor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">context_df</span> <span class="o">=</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
    <span class="n">context_array</span> <span class="o">=</span> <span class="n">context_df</span><span class="p">[</span><span class="n">reward_model</span><span class="o">.</span><span class="n">context_column_order</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">observation_expected_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_probability</span> <span class="ow">in</span> <span class="n">action_probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_array</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
            <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">action_probability</span> <span class="o">*</span> <span class="n">predicted_reward</span>
        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="n">observation_expected_reward</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Scenario<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> 
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>     
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.40</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">10</span><span class="p">},</span> <span class="c1"># only allowed due to exploration     </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.02}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>{'p_fraud': 0.4}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-10</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use the direct method to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>The direct method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We will use the same production logs above and run them through the new policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 15.89,
  &#39;ci_low&#39;: -7.89,
  &#39;mean&#39;: 4.0},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 19.34,
  &#39;ci_low&#39;: -11.12,
  &#39;mean&#39;: 4.11}}
</pre></div>
</div>
</div>
</div>
<p>The direct method estimates that the expected reward per observation for the new policy is slightly better than the logging policy so we would think about rolling out this new policy into an A/B test or production.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
<div class="section" id="dr">
<h2>DR<a class="headerlink" href="#dr" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NoReturn</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>


<span class="k">def</span> <span class="nf">fit_gbdt_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT regressor.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingRegressor</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_gbdt_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge regression.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">Predictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_preprocess_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="c1"># preprocess context</span>
        <span class="n">context_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_column_order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">context_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># preprocess actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possible_actions</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">action_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">one_hot_action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>

        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">one_hot_action_values</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">values</span>

        <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_data</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">fit_gbdt_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_stats</span> <span class="o">=</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="c1"># train a model that predicts reward given (context, action)</span>
    <span class="n">reward_model</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
    <span class="n">reward_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">evaluate_raw</span><span class="p">(</span>
                <span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span>
            <span class="p">)</span>
        <span class="p">]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="p">:</span> <span class="n">Predictor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">context_df</span> <span class="o">=</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
    <span class="n">context_array</span> <span class="o">=</span> <span class="n">context_df</span><span class="p">[</span><span class="n">reward_model</span><span class="o">.</span><span class="n">context_column_order</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">observation_expected_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">processed_context</span> <span class="o">=</span> <span class="n">context_array</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c1"># first compute the left hand term, which is the direct method</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_probability</span> <span class="ow">in</span> <span class="n">action_probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">processed_context</span><span class="p">,</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
            <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">action_probability</span> <span class="o">*</span> <span class="n">predicted_reward</span>

        <span class="c1"># then compute the right hand term, which is similar to IPS</span>
        <span class="n">logged_action</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span>
        <span class="n">new_action_probability</span> <span class="o">=</span> <span class="n">action_probabilities</span><span class="p">[</span><span class="n">logged_action</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">new_action_probability</span> <span class="o">/</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_prob&quot;</span><span class="p">]</span>
        <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">processed_context</span><span class="p">,</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
        <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">predicted_reward</span><span class="p">)</span>

        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="n">observation_expected_reward</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h3>Scenario<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> 
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>     
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.40</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">10</span><span class="p">},</span> <span class="c1"># only allowed due to exploration     </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.02}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>{'p_fraud': 0.4}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-10</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use the doubly robust method to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>The doubly robust method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We will use the same production logs above and run them through the new policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 10.58,
  &#39;ci_low&#39;: -8.24,
  &#39;mean&#39;: 1.17},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 52.14,
  &#39;ci_low&#39;: -110.68,
  &#39;mean&#39;: -29.27}}
</pre></div>
</div>
</div>
</div>
<p>The doubly robust method estimates that the expected reward per observation for the new policy is much worse than the logging policy so we wouldn’t roll out this new policy into an A/B test or production and instead should test some different policies offline.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "RecoHut-Projects/ope-rec",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="US773842_Off_Policy_Evaluation.html">
   Off-Policy Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L978189_Counterfactual_Policy_Evaluation.html">
   Counterfactual policy evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L845850_Evaluating_non_stationary_policies.html">
   Evaluating non-stationary policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L036590_Online_vs_Offline_Evaluation.html">
   Online vs Offline Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L410896_Open_Bandit_Dataset.html">
   Open Bandit Dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="L910228_OBP_Library.html">
   OBP Library
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="C862189_Direct_method.html">
   Direct method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C674274_Inverse_Propensity_Weighting.html">
   Inverse Propensity Weighting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C235655_Doubly_Robust.html">
   Doubly Robust
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T601959_Evaluating_a_New_Fraud_Policy_with_DM%2C_IPW%2C_and_DR_Methods.html">
   Evaluating a New Fraud Policy with DM, IPW, and DR Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Batch Learning from Bandit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="US773842_Off_Policy_Evaluation.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="" title=""><span class="show-for-sr"></span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                        <li data-tippy-content="Download PDF" onClick="window.print()"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/RecoHut-Projects/ope-rec/tree/main/docs/T601959_Evaluating_a_New_Fraud_Policy_with_DM_IPW_and_DR_Methods.ipynb" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="" data-urlpath="" data-branch=>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "T601959_Evaluating_a_New_Fraud_Policy_with_DM_IPW_and_DR_Methods";
                const repoURL = "";
                const urlPath = "";
                const branch = ""
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
    
  </body>
</html>