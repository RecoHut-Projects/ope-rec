
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluating a New Fraud Policy with DM, IPW, and DR Methods &#8212; ope-rec</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html" />
    <link rel="prev" title="OBP Library Workshop Tutorials" href="T966055_OBP_Library_Workshop_Tutorials.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">ope-rec</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="US773842_Off_Policy_Evaluation.html">
   Off-Policy Evaluation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L978189_Counterfactual_Policy_Evaluation.html">
   Counterfactual policy evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L845850_Evaluating_non_stationary_policies.html">
   Evaluating non-stationary policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L036590_Online_vs_Offline_Evaluation.html">
   Online vs Offline Evaluation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L410896_Open_Bandit_Dataset.html">
   Open Bandit Dataset
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tools
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L910228_OBP_Library.html">
   OBP Library
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C862189_Direct_method.html">
   Direct method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C674274_Inverse_Propensity_Weighting.html">
   Inverse Propensity Weighting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C235655_Doubly_Robust.html">
   Doubly Robust
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T167249_Offline_Policy_Evaluation_with_VW_Command_Line.html">
   Offline Policy Evaluation with VW Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T966055_OBP_Library_Workshop_Tutorials.html">
   OBP Library Workshop Tutorials
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Evaluating a New Fraud Policy with DM, IPW, and DR Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html">
   Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705904_Evaluation_of_Multiple_Off_Policy_Estimators_on_Synthetic_Dataset.html">
   Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T890478_Batch_Learning_from_Bandit_Feedback_%28BLBF%29.html">
   Batch Learning from Bandit Feedback
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T902666_Evaluating_the_Robustness_of_Off_Policy_Evaluation.html">
   Evaluating the Robustness of Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471827_Adaptive_Estimator_Selection_for_Off_Policy_Evaluation.html">
   Adaptive Estimator Selection for Off-Policy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T792262_Optimal_Off_Policy_Evaluation_from_Multiple_Logging_Policies.html">
   Optimal Off-Policy Evaluation from Multiple Logging Policies
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T601959_Evaluating_a_New_Fraud_Policy_with_DM,_IPW,_and_DR_Methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/ope-rec/main?urlpath=tree/docs/T601959_Evaluating_a_New_Fraud_Policy_with_DM,_IPW,_and_DR_Methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/ope-rec/blob/main/docs/T601959_Evaluating_a_New_Fraud_Policy_with_DM,_IPW,_and_DR_Methods.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ips">
   IPS
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scenario">
     Scenario
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dm">
   DM
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Scenario
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dr">
   DR
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Scenario
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="evaluating-a-new-fraud-policy-with-dm-ipw-and-dr-methods">
<h1>Evaluating a New Fraud Policy with DM, IPW, and DR Methods<a class="headerlink" href="#evaluating-a-new-fraud-policy-with-dm-ipw-and-dr-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="ips">
<h2>IPS<a class="headerlink" href="#ips" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statistics</span>


<span class="n">P95_Z_SCORE</span> <span class="o">=</span> <span class="mf">1.96</span>


<span class="k">def</span> <span class="nf">compute_list_stats</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute mean and P95 CI of mean for a list of floats.&quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">stdev</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">ci_low</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span> <span class="o">-</span> <span class="n">P95_Z_SCORE</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">std_dev</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">ci_high</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">P95_Z_SCORE</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">std_dev</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;ci_low&quot;</span><span class="p">:</span> <span class="n">ci_low</span><span class="p">,</span> <span class="s2">&quot;ci_high&quot;</span><span class="p">:</span> <span class="n">ci_high</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="p">(</span>
            <span class="n">action_probabilities</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]]</span> <span class="o">/</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_prob&quot;</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">*</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="scenario">
<h3>Scenario<a class="headerlink" href="#scenario" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>    
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use IPS to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>IPS requires that we know P(action | context) for the new policy. We can easily describe our new policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We can now get the probability that the new policy takes the same action that was taken in the production logs above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span><span class="p">[</span><span class="s2">&quot;new_action_prob&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">action_probabilities</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]],</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
      <th>new_action_prob</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
      <td>0.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
      <td>0.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
      <td>0.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We see that the new policy lets through a fraud example (row: 3) at a much higher probability. This should make the new model get penalized in offline evaluation. We also see that for row: 0, the new model has a 90% chance of allowing the transaction, but we don’t have the counterfactual knowledge of whether or not this would have been a non-fraud transaction since in production this transaction was blocked. This demonstrates one of the drawbacks of offline policy evaluation, but with more data we’d ideally see a different action taken in the same situation (due to exploration).</p>
<p>Now we will score the new model using IPS:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 17.59,
  &#39;ci_low&#39;: -11.19,
  &#39;mean&#39;: 3.2},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 46.68,
  &#39;ci_low&#39;: -109.88,
  &#39;mean&#39;: -31.6}}
</pre></div>
</div>
</div>
</div>
<p>The expected reward per observation for the new policy is much worse than the logging policy (due to the observation that allowed fraud to go through (row: 3)) so we wouldn’t roll out this new policy into an A/B test or production and instead should test some different policies offline.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
<div class="section" id="dm">
<h2>DM<a class="headerlink" href="#dm" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NoReturn</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>


<span class="k">def</span> <span class="nf">fit_gbdt_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT regressor.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingRegressor</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_gbdt_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge regression.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">Predictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_preprocess_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="c1"># preprocess context</span>
        <span class="n">context_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_column_order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">context_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># preprocess actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possible_actions</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">action_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">one_hot_action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>

        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">one_hot_action_values</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">values</span>

        <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_data</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">fit_gbdt_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_stats</span> <span class="o">=</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="c1"># train a model that predicts reward given (context, action)</span>
    <span class="n">reward_model</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
    <span class="n">reward_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">evaluate_raw</span><span class="p">(</span>
                <span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span>
            <span class="p">)</span>
        <span class="p">]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="p">:</span> <span class="n">Predictor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">context_df</span> <span class="o">=</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
    <span class="n">context_array</span> <span class="o">=</span> <span class="n">context_df</span><span class="p">[</span><span class="n">reward_model</span><span class="o">.</span><span class="n">context_column_order</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">observation_expected_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_probability</span> <span class="ow">in</span> <span class="n">action_probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_array</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
            <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">action_probability</span> <span class="o">*</span> <span class="n">predicted_reward</span>
        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="n">observation_expected_reward</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Scenario<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> 
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>     
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.40</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">10</span><span class="p">},</span> <span class="c1"># only allowed due to exploration     </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.02}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>{'p_fraud': 0.4}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-10</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use the direct method to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>The direct method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We will use the same production logs above and run them through the new policy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 15.89,
  &#39;ci_low&#39;: -7.89,
  &#39;mean&#39;: 4.0},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 19.34,
  &#39;ci_low&#39;: -11.12,
  &#39;mean&#39;: 4.11}}
</pre></div>
</div>
</div>
</div>
<p>The direct method estimates that the expected reward per observation for the new policy is slightly better than the logging policy so we would think about rolling out this new policy into an A/B test or production.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
<div class="section" id="dr">
<h2>DR<a class="headerlink" href="#dr" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NoReturn</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>


<span class="k">def</span> <span class="nf">fit_gbdt_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT regressor.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingRegressor</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_gbdt_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn GBDT classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_regression</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge regression.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">mse_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">mse_test</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;mse_train&quot;</span><span class="p">:</span> <span class="n">mse_train</span><span class="p">,</span> <span class="s2">&quot;mse_test&quot;</span><span class="p">:</span> <span class="n">mse_test</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_ridge_classification</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Off the shelf sklearn Ridge classifier.&quot;&quot;&quot;</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">acc_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

    <span class="n">acc_test</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">X_test</span> <span class="ow">and</span> <span class="n">y_test</span><span class="p">:</span>
        <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;acc_train&quot;</span><span class="p">:</span> <span class="n">acc_train</span><span class="p">,</span> <span class="s2">&quot;acc_test&quot;</span><span class="p">:</span> <span class="n">acc_test</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">Predictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_preprocess_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
        <span class="c1"># preprocess context</span>
        <span class="n">context_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_column_order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">context_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># preprocess actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">possible_actions</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">action_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">one_hot_action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>

        <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">context_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">one_hot_action_values</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">values</span>

        <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_data</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">fit_gbdt_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_stats</span> <span class="o">=</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>

    <span class="c1"># train a model that predicts reward given (context, action)</span>
    <span class="n">reward_model</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
    <span class="n">reward_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_raw</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bootstrap_samples</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">evaluate_raw</span><span class="p">(</span>
                <span class="n">df</span><span class="p">,</span> <span class="n">action_prob_function</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span>
            <span class="p">)</span>
        <span class="p">]</span>

    <span class="n">logging_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;logging_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="n">new_policy_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;new_policy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;expected_reward_logging_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">logging_policy_rewards</span><span class="p">),</span>
        <span class="s2">&quot;expected_reward_new_policy&quot;</span><span class="p">:</span> <span class="n">compute_list_stats</span><span class="p">(</span><span class="n">new_policy_rewards</span><span class="p">),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">evaluate_raw</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">action_prob_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">sample</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="p">:</span> <span class="n">Predictor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

    <span class="n">tmp_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">sample</span> <span class="k">else</span> <span class="n">df</span>

    <span class="n">context_df</span> <span class="o">=</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
    <span class="n">context_array</span> <span class="o">=</span> <span class="n">context_df</span><span class="p">[</span><span class="n">reward_model</span><span class="o">.</span><span class="n">context_column_order</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">cum_reward_new_policy</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">observation_expected_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">processed_context</span> <span class="o">=</span> <span class="n">context_array</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c1"># first compute the left hand term, which is the direct method</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="n">action_prob_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_probability</span> <span class="ow">in</span> <span class="n">action_probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">processed_context</span><span class="p">,</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
            <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">action_probability</span> <span class="o">*</span> <span class="n">predicted_reward</span>

        <span class="c1"># then compute the right hand term, which is similar to IPS</span>
        <span class="n">logged_action</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span>
        <span class="n">new_action_probability</span> <span class="o">=</span> <span class="n">action_probabilities</span><span class="p">[</span><span class="n">logged_action</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">new_action_probability</span> <span class="o">/</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_prob&quot;</span><span class="p">]</span>
        <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">action_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">processed_context</span><span class="p">,</span> <span class="n">one_hot_action</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>
        <span class="n">predicted_reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">observation_expected_reward</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">predicted_reward</span><span class="p">)</span>

        <span class="n">cum_reward_new_policy</span> <span class="o">+=</span> <span class="n">observation_expected_reward</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;logging_policy&quot;</span><span class="p">:</span> <span class="n">tmp_df</span><span class="o">.</span><span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
        <span class="s2">&quot;new_policy&quot;</span><span class="p">:</span> <span class="n">cum_reward_new_policy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_df</span><span class="p">),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h3>Scenario<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Assume we have a fraud model in production that blocks transactions if the P(fraud) &gt; 0.05.</p>
<p>Let’s build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/ε = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> 
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>     
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.09</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">20</span><span class="p">},</span> <span class="c1"># only allowed due to exploration </span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">:</span> <span class="mf">0.40</span><span class="p">},</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;allowed&quot;</span><span class="p">,</span> <span class="s2">&quot;action_prob&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">10</span><span class="p">},</span> <span class="c1"># only allowed due to exploration     </span>
<span class="p">])</span>

<span class="n">logs_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>context</th>
      <th>action</th>
      <th>action_prob</th>
      <th>reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'p_fraud': 0.08}</td>
      <td>blocked</td>
      <td>0.9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'p_fraud': 0.03}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'p_fraud': 0.02}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'p_fraud': 0.01}</td>
      <td>allowed</td>
      <td>0.9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>{'p_fraud': 0.09}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>{'p_fraud': 0.4}</td>
      <td>allowed</td>
      <td>0.1</td>
      <td>-10</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s use the doubly robust method to score a more lenient fraud model that blocks transactions only if the P(fraud) &gt; 0.10.</p>
<p>The doubly robust method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.10</span>
    <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="s2">&quot;p_fraud&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">}</span>    
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;allowed&quot;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;blocked&quot;</span><span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We will use the same production logs above and run them through the new policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">logs_df</span><span class="p">,</span> <span class="n">action_probabilities</span><span class="p">,</span> <span class="n">num_bootstrap_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;expected_reward_logging_policy&#39;: {&#39;ci_high&#39;: 10.58,
  &#39;ci_low&#39;: -8.24,
  &#39;mean&#39;: 1.17},
 &#39;expected_reward_new_policy&#39;: {&#39;ci_high&#39;: 52.14,
  &#39;ci_low&#39;: -110.68,
  &#39;mean&#39;: -29.27}}
</pre></div>
</div>
</div>
</div>
<p>The doubly robust method estimates that the expected reward per observation for the new policy is much worse than the logging policy so we wouldn’t roll out this new policy into an A/B test or production and instead should test some different policies offline.</p>
<p>However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it’s might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/ope-rec",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T966055_OBP_Library_Workshop_Tutorials.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">OBP Library Workshop Tutorials</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T874693_Evaluating_Standard_Off_Policy_Estimators_with_Small_Sample_Open_Bandit_Dataset.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>